{
  "arch_layout": [
    "m4",
    [
      "T22"
    ],
    "m4"
  ],
  "attn_cfg": {
    "num_heads": [
      16,
      16
    ],
    "rotary_emb_dim": [
      32,
      48
    ],
    "window_size": [
      1023,
      -1
    ]
  },
  "d_intermediate": [
    0,
    4096
  ],
  "d_model": [
    1024,
    1536
  ],
  "ssm_cfg": {
    "chunk_size": 256,
    "d_conv": 4,
    "d_state": 128,
    "expand": 2
  },
  "tie_embeddings": false,
  "vocab_size": 256
}
